{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data extraction using Athena from S3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1188,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import io\n",
    "import re\n",
    "import time\n",
    "import pickle \n",
    "\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.width', 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Session(region_name='eu-west-2')\n"
     ]
    }
   ],
   "source": [
    "params = {\n",
    "    'region': 'eu-west-2', #update region\n",
    "    'database': '*****_staging', #update database\n",
    "    'bucket': '******', #update bucket\n",
    "    'path': 'athena-output',\n",
    "    'query': \"***************\" #Update Query to fetch the records\n",
    "}\n",
    "session = boto3.session.Session(profile_name=None)\n",
    "print(session)\n",
    "\n",
    "def athena_query(client, params):\n",
    "    \n",
    "    response = client.start_query_execution(\n",
    "        QueryString=params['query'],\n",
    "        QueryExecutionContext={\n",
    "            'Database': params['database']\n",
    "        },\n",
    "        ResultConfiguration={\n",
    "            'OutputLocation': 's3://' + params['bucket'] + '/' + params['path']\n",
    "        },\n",
    "        WorkGroup = '*********', #update workgroup\n",
    "    )\n",
    "    return response\n",
    "\n",
    "def athena_to_s3(session, params, max_execution = 1000000):\n",
    "    client = session.client('athena', region_name=params[\"region\"])\n",
    "    execution = athena_query(client, params)\n",
    "    execution_id = execution['QueryExecutionId']\n",
    "    state = 'QUEUED'\n",
    "    while (max_execution > 0 and state in ['RUNNING', 'QUEUED']):\n",
    "        max_execution = max_execution - 1\n",
    "        response = client.get_query_execution(QueryExecutionId = execution_id)\n",
    "        if 'QueryExecution' in response and \\\n",
    "                'Status' in response['QueryExecution'] and \\\n",
    "                'State' in response['QueryExecution']['Status']:\n",
    "            state = response['QueryExecution']['Status']['State']\n",
    "            print(state)\n",
    "            if state == 'FAILED':\n",
    "                return False\n",
    "            elif state == 'SUCCEEDED':\n",
    "                s3_path = response['QueryExecution']['ResultConfiguration']['OutputLocation']\n",
    "                filename = re.findall('.*\\/(.*)', s3_path)[0]\n",
    "                print(filename)\n",
    "                return filename\n",
    "        time.sleep(1)\n",
    "    return False\n",
    "\n",
    "def s3_to_pandas(session, params, s3_filename):    \n",
    "    s3client = session.client('s3')\n",
    "    obj = s3client.get_object(Bucket=params['bucket'],\n",
    "                              Key=params['path'] + '/' + s3_filename)\n",
    "    df = pd.read_csv(io.BytesIO(obj['Body'].read()))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time \n",
    "s3_filename = athena_to_s3(session, params)\n",
    "df = s3_to_pandas(session, params, s3_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25721403, 21)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save Data to pickle file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_pickle('data_all.pkl')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
