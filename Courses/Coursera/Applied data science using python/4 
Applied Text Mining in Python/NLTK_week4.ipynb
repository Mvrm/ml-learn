{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.corpus import wordnet as wn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.14285714285714285"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use path length in wordnet to find word similarity\n",
    "# find sense of words via synonym set\n",
    "# n=noun, 01=synonym set for first meaning of the word\n",
    "deer = wn.synset('deer.n.01')\n",
    "deer\n",
    "\n",
    "elk = wn.synset('elk.n.01')\n",
    "deer.path_similarity(elk)\n",
    "\n",
    "horse = wn.synset('horse.n.01')\n",
    "deer.path_similarity(horse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7726998936065773"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use an information criteria to find word similarity\n",
    "from nltk.corpus import wordnet_ic\n",
    "brown_ic = wordnet_ic.ic('ic-brown.dat')\n",
    "deer.lin_similarity(elk, brown_ic)\n",
    "\n",
    "deer.lin_similarity(horse, brown_ic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('mrs', 'hussey'),\n",
       " ('o', 'clock'),\n",
       " ('dough', 'boy'),\n",
       " ('cape', 'horn'),\n",
       " ('moby', 'dick'),\n",
       " ('town', 'ho'),\n",
       " ('try', 'works'),\n",
       " ('new', 'bedford'),\n",
       " ('ha', 'ha'),\n",
       " ('king', 'post')]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use NLTK Collocation and Association Measures\n",
    "from nltk.collocations import *\n",
    "# load some text for examples\n",
    "from nltk.book import *\n",
    "# text1 is the book \"Moby Dick\"\n",
    "\n",
    "# extract just the words without numbers and sentence marks and make them lower case\n",
    "text = [w.lower() for w in list(text1) if w.isalpha()]\n",
    "\n",
    "bigram_measures = nltk.collocations.BigramAssocMeasures()\n",
    "finder = BigramCollocationFinder.from_words(text)\n",
    "finder.nbest(bigram_measures.pmi,10)\n",
    "\n",
    "# find all the bigrams with occurrence of at least 10, this modifies our \"finder\" object\n",
    "finder.apply_freq_filter(10)\n",
    "finder.nbest(bigram_measures.pmi,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'tuple'>\n",
      "[(0, 2), (2000, 1), (4000, 4), (6000, 1), (8000, 97), (10000, 1)]\n",
      "[(0, '0.012*\"mr\" + 0.012*\"elinor\" + 0.010*\"could\" + 0.010*\"mariann\" + 0.009*\"would\" + 0.007*\"said\" + 0.006*\"everi\" + 0.006*\"one\" + 0.006*\"sister\" + 0.005*\"much\"'), (1, '0.014*\"whale\" + 0.008*\"one\" + 0.006*\"like\" + 0.005*\"ship\" + 0.005*\"upon\" + 0.005*\"ye\" + 0.005*\"man\" + 0.005*\"sea\" + 0.004*\"ahab\" + 0.004*\"boat\"'), (2, '0.010*\"govern\" + 0.010*\"nation\" + 0.009*\"peopl\" + 0.007*\"us\" + 0.007*\"state\" + 0.006*\"upon\" + 0.005*\"power\" + 0.005*\"must\" + 0.005*\"great\" + 0.005*\"countri\"'), (3, '0.023*\"join\" + 0.023*\"part\" + 0.019*\"lol\" + 0.015*\"hi\" + 0.014*\"unto\" + 0.011*\"said\" + 0.008*\"action\" + 0.007*\"son\" + 0.007*\"hey\" + 0.006*\"thou\"')]\n"
     ]
    }
   ],
   "source": [
    "# Working with Latent Dirichlet Allocation (LDA) in Python\n",
    "# Several packages available, such as gensim and lda. Text needs to be\n",
    "# preprocessed: tokenizing, normalizing such as lower-casing, stopword\n",
    "# removal, stemming, and then transforming into a (sparse) matrix for\n",
    "# word (bigram, etc) occurences.\n",
    "# generate a set of preprocessed documents\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.book import *\n",
    "\n",
    "len(stopwords.words('english'))\n",
    "\n",
    "stopwords.words('english')\n",
    "\n",
    "# extract just the stemmed words without numbers and sentence marks and make them lower case\n",
    "p_stemmer = PorterStemmer()\n",
    "sw = stopwords.words('english')\n",
    "doc1 = [p_stemmer.stem(w.lower()) for w in list(text1) if w.isalpha() and not w.lower() in sw]\n",
    "doc2 = [p_stemmer.stem(w.lower()) for w in list(text2) if w.isalpha() and not w.lower() in sw]\n",
    "doc3 = [p_stemmer.stem(w.lower()) for w in list(text3) if w.isalpha() and not w.lower() in sw]\n",
    "doc4 = [p_stemmer.stem(w.lower()) for w in list(text4) if w.isalpha() and not w.lower() in sw]\n",
    "doc5 = [p_stemmer.stem(w.lower()) for w in list(text5) if w.isalpha() and not w.lower() in sw]\n",
    "doc_set = [doc1, doc2, doc3, doc4, doc5]\n",
    "\n",
    "# under Windows this generates a warning\n",
    "import gensim\n",
    "from gensim import corpora, models\n",
    "\n",
    "dictionary = corpora.Dictionary(doc_set)\n",
    "dictionary\n",
    "\n",
    "# transform each document into a bag of words\n",
    "corpus = [dictionary.doc2bow((doc)) for doc in doc_set]\n",
    "\n",
    "# The corpus contains the 5 documents\n",
    "# each document is a list of indexed features and occurrence count (freq)\n",
    "print(type(corpus))\n",
    "print(type(corpus[0]))\n",
    "print(type(corpus[0][0]))\n",
    "print(corpus[0][::2000])\n",
    "\n",
    "# let's try 4 topics for our 5 documents\n",
    "# 50 passes takes quite a while, let's try less\n",
    "ldamodel = gensim.models.ldamodel.LdaModel(corpus, num_topics=4, id2word=dictionary, passes=10)\n",
    "\n",
    "print(ldamodel.print_topics(num_topics=4, num_words=10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
